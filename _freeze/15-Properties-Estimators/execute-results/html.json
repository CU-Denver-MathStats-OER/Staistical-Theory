{
  "hash": "8f57fcd7805c608fe33c9d29a2a2b9e6",
  "result": {
    "markdown": "---\ntitle: '4.3: Properties of Estimators'\n#author: 'Adam Spiegler, University of Colorado Denver'\n#execute:\n#  eval: false\noutput:\n  html_document:\n#    toc: yes\n#    toc_depth: 1\n#    theme: cerulean\n#jupyter:\n#  kernelspec:\n#    display_name: R\n#    language: R\n#    name: ir\n#output:\n#    ipynbdocument::ipynb_document\n---\n\n\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/CU-Denver-MathStats-OER/Statistical-Theory/blob/main/Chap4/15-Properties-Estimators.ipynb){target=\"_blank\"} <nbsp>\n\n![Credit: Linnaea Mallette, [\"Two Paths Background\"](https://www.publicdomainpictures.net/en/view-image.php?image=107495&picture=two-paths-background), [CC-0-1.0](https://creativecommons.org/publicdomain/zero/1.0/)](https://www.publicdomainpictures.net/pictures/110000/velka/two-paths-background.jpg){fig-align=\"left\" width=50% fig-alt=\"Two Paths\"} <nbsp>\n\n\nIn this section, we continue our study of estimators of population parameters. An <span style=\"color:dodgerblue\">**estimator**</span>, denoted $\\color{dodgerblue}{\\hat{\\theta}}$, is a formula or rule that we use to estimate the value of an unknown population parameter $\\theta$. For a single parameter $\\theta$, there are many (possibly infinite) different estimators $\\hat{\\theta}$ from which to choose from. We have more deeply investigated two particularly useful methods: [maximum likelihood estimates (MLE)](13-Estimation-MLE.qmd){target=\"_blank\"} and [method of moments](14-Estimation-MOM.qmd){target=\"_blank\"} estimation. \n\nWe can think of different estimators, $\\hat{\\theta}$, as different paths attempting to arrive at the same destination, the value of $\\theta$. Different statisticians might prefer different paths, which path is optimal? Sometimes different methods lead to the same result, and sometimes they differ. When they differ, how do we decide which estimate is best? \n\n## Question 1\n\n---\n\n\nSuppose our population parameter of interest is the center of a dart board. We use four different methods for throwing darts and the results of those four different methods are displayed in @fig-dart-board.\n\n\n::: {#fig-dart-board layout-nrow=1}\n![Dart Method 1](Images/15fig-dart1.png)\n\n\n![Dart Method 2](Images/15fig-dart2.png)\n\n![Dart Method 3](Images/15fig-dart3.png)\n\n![Dart Method 4](Images/15fig-dart4.png)\n\n\n\nComparing the results of four different methods for throwing darts.  <br> Credit: [Arbeck](https://commons.wikimedia.org/wiki/File:Accuracy_and_Precision.svg), [CC BY 4.0](https://creativecommons.org/licenses/by/4.0), via Wikimedia Commons\n:::\n\n\n### Question 1a\n\n---\n\nRank the results of the four dart methods in terms of accuracy, from most to least accurate. Explain your reasoning.\n\n\n#### Solution to Question 1a\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n### Question 1b\n\n---\n\nRank the results of the four dart methods in terms of precision, from most to least precise. Explain your reasoning.\n\n\n#### Solution to Question 1b\n\n---\n\n\n<br>  \n<br>  \n<br>  \n\n\n### Question 1c\n\n---\n\nRank the results of the four dart methods from best to worst overall. Explain your reasoning.\n\n\n#### Solution to Question 1c\n\n---\n\n\n<br>  \n<br>  \n<br>  \n\n\n### Question 1d\n\n---\n\nFour different sampling distributions of the results of the four dart throwing methods are plotted in @fig-dartdist. The location of the <span style=\"color:tomato\">population parameter</span> (the center of the dart board) is indicated by the <span style=\"color:tomato\">dashed red line</span>. The <span style=\"color:dodgerblue\">mean of the sampling distribution</span> is indicated by the <span style=\"color:dodgerblue\">solid blue vertical line</span>. Match each of the distributions labeled A-D below to one of the four dart boards displayed in [Question 1].\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {#fig-dartdist .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](15-Properties-Estimators_files/figure-html/fig-dartdist-1.png){#fig-dartdist-1 width=672}\n:::\n\n::: {.cell-output-display}\n![](15-Properties-Estimators_files/figure-html/fig-dartdist-2.png){#fig-dartdist-2 width=672}\n:::\n\n::: {.cell-output-display}\n![](15-Properties-Estimators_files/figure-html/fig-dartdist-3.png){#fig-dartdist-3 width=672}\n:::\n\n::: {.cell-output-display}\n![](15-Properties-Estimators_files/figure-html/fig-dartdist-4.png){#fig-dartdist-4 width=672}\n:::\n\nMatching Distributions to Dart Boards\n:::\n\n\n\n#### Solution to Question 1d\n\n---\n\n\n- Dart Distribution A matches dart method ??.\n- Dart Distribution B matches dart method ??.\n- Dart Distribution C matches dart method ??.\n- Dart Distribution D matches dart method ??.\n\n<br>  \n<br>  \n\n\n# Comparing Estimators\n\n\n---\n\nAt first glance, the question of which estimator is best may seem like a simple question. \n\n> The estimator that gives a value closest to the population parameter is best! \n\nHowever, choosing the \"best\" estimator is not as straightforward as simply choosing the estimator that gives the value closest to the actual value. **The parameters we are estimating are unknown values!** We do not know where the center of the dart board is located. We cannot be *certain* which estimator leads to the closest estimate. Different samples will give different estimates even if we use the same formula for the estimator, and each estimate has some uncertainty due to sampling.\n\n  - We might choose the \"best\" method but get \"unlucky\" by randomly selecting a biased sample.\n  - We might choose a \"bad\" method but get \"lucky\" because we picked a good sample.\n\nWe can still choose a method that is more likely to give a better estimate (such as MLE) and/or minimizes the effect of the uncertainty due to sampling.  Which properties are most important to consider depend on many factors. For example:\n\n- What is the parameter we are trying to estimate?\n- What is the shape of the distribution of the population?\n- Who are the statisticians/researchers? Different statisticians might have different goals.\n- How is the estimate going to assessed?  For example, is precision or accuracy more important?\n\n\nThere are many properties of estimators worth considering when deciding between different estimators. In this section, we explore properties relating to **accuracy (bias)**, **precision (variability)**, and  the **mean squared error (MSE)** which takes both bias and variability into consideration. \n\n\n\n## Question 2\n\n---\n\nLet $X$ denote the diastolic blood pressure (in mm Hg) of a randomly selected woman from the Pima Indian Community. The [Pima Indian Community](https://itcaonline.com/member-tribes/salt-river-pima-maricopa-indian-community/){target=\"_blank\"} is mostly located outside of Phoenix, Arizona. The data^[Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C. and Johannes, R. S. (1988) \"Using the ADAP learning algorithm to forecast the onset of diabetes mellitus\". In *Proceedings of the Symposium on Computer Applications in Medical Care *(Washington, 1988), ed. R. A. Greenes, pp. 261â€“265. Los Alamitos, CA: IEEE Computer Society Press.] used in this example is from the data frame `Pima.tr` in the `MASS` package.\n\nIf we suppose blood pressure is normally distributed, then we have $X \\sim N(\\mu, \\sigma)$. We would like to decide which estimator is best for the population mean $\\mu$. We pick a random sample of $n=20$ women in the code cell below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)  # load MASS package to access data\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)  # fix randomization of sample\nx <- sample(Pima.tr$bp, size=20)  # pick a random sample of 20 blood pressures\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  58  64  64  80  68  76  72 102  68  52  90  70  56  62  74  74  76  82  85\n[20]  52\n```\n:::\n:::\n\n\nThe random sample of diastolic blood pressure values is\n$$\\mathbf{x} = (58, 64, 64, 80, 68, 76, 72, 102, 68, 52, 90, 70, 56, 62, 74, 74, 76, 82, 85, 52).$$\nConsider the following estimates for the population mean $\\mu$:\n\n<span style=\"color:dodgerblue\">1. The **sample mean**: $\\hat{\\mu}_1 = \\bar{X} = \\dfrac{\\sum_{i=1}^n X_i}{n}$.</span>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. calculate sample mean\nmu.hat1 <- mean(x)\nmu.hat1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 71.25\n```\n:::\n:::\n\n\n\n<span style=\"color:tomato\">2. The **sample median**, denoted $\\hat{\\mu}_2 = \\mbox{median}$.</span>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2. calculate sample median\nmu.hat2 <- median(x)\nmu.hat2 \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 71\n```\n:::\n:::\n\n\n\n<span style=\"color:mediumseagreen\">3. The **mid-range** of the sample, $\\hat{\\mu}_3 = \\dfrac{X_{\\rm{max}} + X_{\\rm{min}}}{2}$.</span>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 3. calculate sample mid range\nmu.hat3 <- (max(x) + min(x)) / 2\nmu.hat3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 77\n```\n:::\n:::\n\n\n\n<span style=\"color:mediumpurple\">4. The **sample trimmed (10%) mean**, $\\bar{x}_{\\rm{tr}(10)}$.</span>\n\n  - We exclude the smallest 10% of the values. The smallest 2 values, 52 and 52, are excluded.\n  - We exclude the largest 10% of the values. The largest 2 values, 102 and 90, are excluded.\n  - We compute the mean of the remaining 16 values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 4. calculate sample trimmed (10%) mean\nmu.hat4 <- mean(x, trim = 0.1)\nmu.hat4\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 70.5625\n```\n:::\n:::\n\n\n\n### Question 2a\n\n---\n\nWhich estimator (mean, median, mid-range, or trimmed mean) do you believe is best? Which estimator do you believe is the worst? Explain your reasoning.\n\n\n\n#### Solution to Question 2a\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n### Question 2b\n\n---\n\nTo help decide which estimator performs best, we can consider the sampling distribution of estimates obtained from many different random samples (each of size $n=20$) chosen independently from the same population. Based on the sampling distributions for $\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $\\hat{\\mu}_3$, and $\\hat{\\mu}_4$  (mean, median, mid-range, and trimmed mean, respectively) in @fig-bpnormal, rank the four estimators from least to most bias and from most to least precise. Explain your reasoning.\n\n\n\n::: {.cell}\n\n:::\n\n::: {#fig-bpnormal .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Distribution of Sample Means](15-Properties-Estimators_files/figure-html/fig-bpnormal-1.png){#fig-bpnormal-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Distribution of Sample Medians](15-Properties-Estimators_files/figure-html/fig-bpnormal-2.png){#fig-bpnormal-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Distribution of Sample Mid-ranges](15-Properties-Estimators_files/figure-html/fig-bpnormal-3.png){#fig-bpnormal-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Distribution of Sample Trimmed Means](15-Properties-Estimators_files/figure-html/fig-bpnormal-4.png){#fig-bpnormal-4 width=672}\n:::\n\nComparing Estimators for Mean Blood Pressure\n:::\n\n\n\n\n#### Solution to Question 2b\n\n---\n\n\n<br>  \n<br>  \n<br>  \n\n\n\n### Question 2c\n\n---\n\nWe still let $X$ denote the diastolic blood pressure (in mm Hg) of a randomly selected woman from the Pima Indian Community. However, now we suppose blood pressure is **uniformly distributed** over the interval $\\lbrack 41.26, 101.26 \\rbrack$. Although a uniform distribution would not make practical sense for blood pressure, we make this assumption in order to investigate how the shape of the population may affect which estimator works best.\n\n\nConsider the sampling distribution of estimates obtained from many different random samples (each of size $n=20$) chosen independently from a  **uniformly distributed population**. Based on the sampling distributions for $\\hat{\\mu}_1$, $\\hat{\\mu}_2$, $\\hat{\\mu}_3$, and $\\hat{\\mu}_4$  (mean, median, mid-range, and trimmed mean, respectively) in @fig-bpunif, rank the four estimators again from least to most bias and from most to least precise. **Compare your updated rankings to those from [Question 2b]. Did your rankings change?**\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {#fig-bpunif .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Distribution of Sample Means](15-Properties-Estimators_files/figure-html/fig-bpunif-1.png){#fig-bpunif-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Distribution of Sample Medians](15-Properties-Estimators_files/figure-html/fig-bpunif-2.png){#fig-bpunif-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Distribution of Sample Mid-ranges](15-Properties-Estimators_files/figure-html/fig-bpunif-3.png){#fig-bpunif-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Distribution of Sample Trimmed Means](15-Properties-Estimators_files/figure-html/fig-bpunif-4.png){#fig-bpunif-4 width=672}\n:::\n\nComparing Estimators for a Uniform Population\n:::\n\n\n\n\n\n#### Solution to Question 2c\n\n---\n\n\n<br>  \n<br>  \n<br>  \n\n\n# Bias of an Estimator\n\n---\n\nNo matter what formula we choose as an estimator, the estimate we obtain will vary from sample to sample. We like an estimator to be, on average, equal to the parameter it is estimating. The <span style=\"color:dodgerblue\">**bias**</span> of an estimator $\\hat{\\theta }$ for parameter $\\theta$ is defined as the difference in the average (expected) value of the estimator and the parameter $\\theta$,\n\n$${\\large \\color{dodgerblue}{\\boxed{ \\mbox{Bias} = E(\\hat{\\theta}) - \\theta.}}}$$\n\n- $\\hat{\\theta}$ is an <span style=\"color:dodgerblue\">**unbiased estimator**</span> if $\\color{dodgerblue}{\\mbox{Bias} = E(\\hat{\\theta}) - \\theta =0}$.\n- If the bias is positive, then on average $\\hat{\\theta}$ gives an overestimate for $\\theta$.\n- If the bias is negative, then on average $\\hat{\\theta}$ gives an underestimate for $\\theta$.\n\n\nEstimates that are perfectly unbiased may be impossible or unreasonable at times. In practice, we are satisfied when estimates are approximately unbiased, or when the bias gets closer and closer to 0 as the sample size, $n$, gets larger. \n\n\n\n## Question 3 {#sec-prop-bias}\n\n---\n\nLet $X \\sim \\mbox{Binom}(n,p)$ with $n$ known and parameter $p$ unknown. Consider the following two estimators for parameter $p$:\n\n1. The usual sample proportion, $\\hat{p} = \\frac{X}{n}$.\n\n2. A modified proportion, $\\tilde{p} = \\frac{X+2}{n+4}$. This is equivalent to adding 4 more trials to the sample, 2 of which are successes.\n\n\n::: {.callout-tip}\nUse [properties of expected value](06-Expected-Value-and-Variance.qmd#sec-prop-exp){target=\"_blank\"} and recall these useful formulas for $X \\sim \\mbox{Binom}(n,p)$,\n\n$$E(X) = np \\quad \\mbox{and} \\quad \\mbox{Var}(X) = np(1-p).$$\n\n:::\n\n### Question 3a\n\n---\n\nDetermine whether the estimator $\\hat{p} = \\frac{X}{n}$ is biased or unbiased.\n\n#### Solution to Question 3a\n\n---\n\n<br>  \n<br>  \n<br>  \n\n### Question 3b\n\n---\n\nDetermine whether the estimator $\\tilde{p} = \\frac{X+2}{n+4}$ is biased or unbiased.\n\n\n#### Solution to Question 3b\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n## Question 4 {#sec-var-bias}\n\n---\n\nLet $X \\sim N(\\mu, \\sigma)$. In [Question 2], we consider several estimators for the parameter $\\mu$. We now consider two possible estimators for the parameter $\\sigma^2$, the variance of the population.\n\n1. Using the estimator $\\displaystyle s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}$.\n\n2. Using the estimator $\\displaystyle \\hat{\\sigma}^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}$.\n\n\n\n### Question 4a\n\n---\n\nProve the following statement:\n\n\nIf $X_1$, $X_2$, $\\ldots$ , $X_n$ are independently and identically distributed random variables with $E(X_i) = \\mu$ and $\\mbox{Var}(X_i) = \\sigma^2$, then \n\n$${\\color{dodgerblue}{\\boxed{E \\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] = (n-1)\\sigma^2.}}}$$\n\n:::{.callout-tip}\nUse the result of [Theorem 15.1] that states the following:\n\n\nIf $X_1$, $X_2$, $\\ldots$ , $X_n$ are independently and identically distributed random variables with $\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$, then \n\n$$\\boxed{ E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n  E \\big[ X_i^2 \\big] - n E \\big[ \\overline{X}^2 \\big]}$$\n\n:::\n\n\n#### Solution to Question 4a\n\n---\n\n**Proof**:\n\nWe first apply [Theorem 15.1] to begin simplifying the expected value of the sum of the squared deviations,\n\n$$E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}}$$\n\nNext we simplify using properties of random variables and summations as follows,\n\n$$\\begin{aligned}\nE\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] &= \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}} & \\mbox{by Theorem 15.1}\\\\\n&=  \\sum_{i=1}^n \\bigg( {\\color{dodgerblue}{ \\mbox{Var} \\big[ X_i \\big] + \\left( E \\big[ X_i \\big] \\right)^2 }} \\bigg) - n \\left( {\\color{tomato}{\\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2}} \\right) & \\mbox{Justification 1 ??}\\\\\n&=  \\sum_{i=1}^n {\\color{dodgerblue}{ \\left( \\sigma^2 + \\mu^2 \\right)}} - n \\left( \\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2 \\right) & \\mbox{Justification 2 ??}\\\\\n&=  \\sum_{i=1}^n\\left( \\sigma^2 + \\mu^2 \\right) - n \\left( {\\color{tomato}{\\frac{\\sigma^2}{n}}} + \\left( {\\color{tomato}{ \\mu }}\\right)^2 \\right) & \\mbox{Justification 3 ??} \\\\\n&= {\\color{dodgerblue}{n(\\sigma^2 + \\mu^2)}} - \\sigma^2 - n\\mu^2 & \\mbox{Justification 4 ??}\\\\\n&= (n-1) \\sigma^2. & \\mbox{Algebraically simplify}\n\\end{aligned}$$\n\nThis concludes our proof!\n\n##### Justifications for Proof\n\n---\n\n- Justification 1:\n\n- Justification 2:\n\n- Justification 3:\n\n- Justification 4:\n\n<br>  \n<br>  \n\n\n### Question 4b\n\n---\n\nDetermine whether the estimator $\\displaystyle s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}$ is biased or unbiased.\n\n:::{.callout-tip}\nIf we apply the theorem we proved in [Question 4a], this question should not require much more additional work!\n:::\n\n#### Solution to Question 4b\n\n---\n\n<br>  \n<br>  \n<br>  \n\n### Question 4c\n\n---\n\nDetermine whether the estimator $\\displaystyle \\hat{\\sigma}^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n}$ is biased or unbiased.\n\n\n#### Solution to Question 4c\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n## Estimating Variance and Standard Deviation\n\n---\n\nThe variance of random variable $X$ is defined as $\\sigma^2=\\mbox{Var} (X) = E\\big[ (X- \\mu)^2 \\big]$. If we pick a random sample $X_1, X_, \\ldots , X_n$ and want to approximate $\\sigma^2$, then a reasonable recipe for estimating $\\sigma^2$ could be to approximate $E \\big[ (X - \\mu)^2 \\big]$ using the following process:\n\n1. Use the sample mean ${\\color{tomato}{\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i}}$ in place of the unknown value of the parameter ${\\color{tomato}{\\mu}}$.\n2. Based on the sample data, calculate the average value of $(X_i - {\\color{tomato}{\\overline{X}}})^2$.\n\n$$\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^n \\left( X_i- \\overline{X} \\right)^2}{n}.$$\n\nHowever, in [Question 4] we showed this estimator is biased. For this reason:\n\n- The unbiased estimator $s^2$ (that has $n-1$ in the denominator) is usually used to estimate the population variance $\\sigma^2$.\n\n$$s^2 = \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}.$$\n\n- And the estimator $s$ (that also has $n-1$ in the denominator) is usually used to estimate the population standard deviation $\\sigma$.\n\n$$s = \\sqrt{ \\dfrac{\\sum_{i=1}^n (X_i - \\overline{X})^2}{n-1}}.$$\n\n- The R commands `sd(x)` and `var(x)` use the formulas for $s$ and $s^2$, respectively, with $n-1$ in the denominator.\n- For large samples, the difference is very minimal whether we use the estimator with $n$ or $n-1$ in the denominator.\n\n::: {.callout-warning}\nAlthough the sample variance $s^2$ is an unbiased estimator for the population variance $\\sigma^2$, the sample standard deviation $s$ is in general a biased estimator for the population variance $\\sigma$ since it is not true that $\\sqrt{E(X)} = E(\\sqrt{X})$.\n:::\n\n\n# Precision of Estimators\n\n---\n\n\nLet $\\hat{\\theta}$ be an estimator for a parameter $\\theta$. We can measure how precise $\\hat{\\theta}$ is by considering how \"spread out\" the estimates obtained by selecting many random samples (each size $n$) and calculating an estimate $\\hat{\\theta}$.  The <span style=\"color:dodgerblue\">**variance of the sampling distribution**, $\\mbox{Var}(\\hat{\\theta})$</span>, measures the <span style=\"color:dodgerblue\">**variability in estimates**</span> due to the uncertainty in random sampling. The <span style=\"color:dodgerblue\">**standard error**</span> of $\\hat{\\theta}$ is the standard deviation of the sampling distribution for $\\hat{\\theta}$ and also commonly used.\n\n- In some cases, we can use theory from probability to derive a formula for $\\mbox{Var}(\\hat{\\theta})$.\n- We can also approximate $\\mbox{Var}(\\hat{\\theta})$ by creating a sampling distribution through simulations.\n\n\n## Question 5 {#sec-mean-eff}\n\n---\n\nLet $X_1, X_2, X_3$ be independent random variables from an identical distribution with mean and variance $\\mu$ and $\\sigma^2$, respectively, and consider two possible estimators for $\\mu$:\n\n- The usual sample mean, $\\hat{\\mu}_1 = \\overline{X} = \\frac{X_1 + X_2 + X_3}{3}$.\n\n- A weighted sample mean, $\\hat{\\mu}_2 = \\frac{1}{6}X_1 + \\frac{1}{3}X_2 + \\frac{1}{2} X_3$.\n\n\n### Question 5a\n\n---\n\nProve both estimators $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$ are unbiased estimators of $\\mu$.\n\n\n#### Solution to Question 5a\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n\n### Question 5b\n\n---\n\nCalculate $\\mbox{Var}( \\hat{\\mu}_1)$ and $\\mbox{Var}( \\hat{\\mu}_2)$, the variances of the estimators $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$. Which estimator is more precise?\n\n\n::: {.callout-tip}\nRecall properties we can apply when finding the [variance of a linear combination of independent random variables](06-Expected-Value-and-Variance.qmd#sec-prop-var){target=\"_blank\"}, and note **the variances will depend on the unknown value of the population variance,** $\\mathbf{\\sigma^2}$.\n:::\n\n#### Solution to Question 5b\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n\n## Efficiency of Unbiased Estimators\n\n---\n\nIf $\\hat{\\theta}_1$ and $\\hat{\\theta}_2$ are both unbiased estimators of $\\theta$, then <span style=\"color:dodgerblue\">$\\hat{\\theta}_1$ is said to be more efficient than $\\hat{\\theta}_2$</span> if ${\\color{dodgerblue}{\\mbox{Var} ( \\hat{\\theta}_1)  < \\mbox{Var} (\\hat{\\theta}_2)}}$. For example, in [Question 5] we show the usual sample mean $\\hat{\\mu}_1=\\overline{X}$ is a more efficient estimator than the weighted mean $\\hat{\\mu}_2$.\n\n## Question 6\n\n---\n\nLet $X \\sim \\mbox{Binom}(n,p)$ with $n$ known and parameter $p$ unknown. Consider the following two estimators for parameter $p$:\n\n1. The usual sample proportion, $\\hat{p} = \\frac{X}{n}$.\n\n2. A modified proportion, $\\tilde{p} = \\frac{X+2}{n+4}$. \n\n\nRecall in [Question 3] we determined $\\hat{p}$ is an unbiased estimator for $p$ while $\\tilde{p}$ is a slightly biased estimator.\n\n\n::: {.callout-tip}\nUse [properties of variance](06-Expected-Value-and-Variance.qmd#sec-prop-var){target=\"_blank\"} and recall these useful formulas for $X \\sim \\mbox{Binom}(n,p)$,\n\n$$E(X) = np \\quad \\mbox{and} \\quad \\mbox{Var}(X) = np(1-p).$$\n\n:::\n\n### Question 6a\n\n---\n\nFind $\\mbox{Var}(\\hat{p}) = \\mbox{Var} \\left( \\frac{X}{n} \\right)$. Your answer will depend on the sample size $n$ and the parameter $p$.\n\n#### Solution to Question 6a\n\n---\n\n<br>  \n<br>  \n<br>  \n\n### Question 6b\n\n---\n\nFind $\\mbox{Var}(\\tilde{p}) = \\mbox{Var} \\left( \\frac{X+2}{n+4} \\right)$. Your answer will depend on the sample size $n$ and the parameter $p$.\n\n\n#### Solution to Question 6b\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n# Mean Squared Error\n\n---\n\nWe have explored bias and variability of estimators. It is not always possible or reasonable to use an unbiased estimator. Moreover, in some cases an estimator with a little bit of bias and very little variability might be preferred over an unbiased estimator that has a lot of variability. Choosing which estimator is preferred often involves a trade-off between bias and variability. \n\nThe <span style=\"color:dodgerblue\">**Mean Squared Error (MSE)**</span> of an estimator $\\hat{\\theta}$ measures the average squared distance between the estimator and the parameter $\\theta$, \n\n$${\\color{dodgerblue}{\\mbox{MSE} \\big[ \\hat{\\theta} \\big] = E \\big[ (\\hat{\\theta}-\\theta)^2 \\big]}}.$$ \n\n- **The MSE is a criterion that takes into account both the bias and variability of an estimator!**\n- In the [Appendix](#sec-append) we prove [Theorem 15.3] that gives the relation of the MSE to the variance and bias:\n\n\n$$\\boxed{\\large {\\color{dodgerblue}{ \\mbox{MSE} \\big[ \\hat{\\theta} \\big] }} = {\\color{tomato}{\\mbox{Var} \\big[ \\hat{\\theta} \\big]}} + {\\color{mediumseagreen}{\\left( \\mbox{Bias}(\\hat{\\theta}) \\right)^2. }}}$$\n\n- In the special case where $\\hat{\\theta}$ is an unbiased estimator, then $\\mbox{MSE} \\big[ \\hat{\\theta} \\big]  =\\mbox{Var} \\big[ \\hat{\\theta} \\big]$.\n\n\n## Question 7\n\n---\n\nLet $X \\sim \\mbox{Binom}(n,p)$ with $n$ known and parameter $p$ unknown. Consider the following two estimators for parameter $p$:\n\n1. The usual sample proportion, $\\hat{p} = \\frac{X}{n}$.\n\n2. A modified proportion, $\\tilde{p} = \\frac{X+2}{n+4}$. \n\n\nUsing previous results regarding the bias and variability of the estimators we derived in [Question 3] and [Question 6], respectively, answer [Question 7a] and [Question 7b] to compare the MSE of the estimators.\n\n\n### Question 7a\n\n---\n\nGive a formula for $\\mbox{MSE}(\\hat{p})$. Your answer will depend on the sample size $n$ and the parameter $p$.\n\n\n#### Solution to Question 7a\n\n---\n\n<br>  \n<br>  \n<br>  \n\n### Question 7b\n\n---\n\nGive a formula for $\\mbox{MSE}(\\tilde{p})$. Your answer will depend on the sample size $n$ and the parameter $p$.\n\n\n#### Solution to Question 7b\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n\n## Choosing a Sample Proportion {#sec-compare-prop}\n\n---\n\nIn the case of $X \\sim \\mbox{Binom}(n,p)$ with $n$ known and parameter $p$ unknown, we have considered two possible estimators for the population proportion $p$.\n\n\n1. The usual sample proportion, $\\hat{p} = \\frac{X}{n}$.\n    - This estimator makes the most practical sense.\n    - This is the estimator obtained using MLE or MoM.\n    - $\\hat{p}$ is unbiased.\n    - But $\\hat{p}$ can be less precise depending on the value of $p$.\n\n\n2. A modified proportion, $\\tilde{p} = \\frac{X+2}{n+4}$. \n    - $\\tilde{p}$ is biased, but this may not be a problem:\n      - As $n$ gets larger and larger, the bias of this estimator gets smaller and smaller.\n      - The bias is towards $0.5$, so if $p$ is close to $0.5$ this is not a big issue.\n    - $\\tilde{p}$ is a more precise estimator when $p$ is not close to 0 or 1.\n\n\nFor example if $n=16$, we have $X \\sim \\mbox{Binom}(16,p)$. @fig-mse compares the values of $\\mbox{MSE}(\\hat{p})$ and $\\mbox{MSE}(\\tilde{p})$ for $n=16$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- seq(0, 1, length.out = 100)  # values of p\nn <- 16  # sample size\n\n# formula for MSE of p-hat\nmse.phat <- (p * (1 - p)) / n\n\n# formula for MSE of p-tilde\nmse.ptilde <- (n * p * (1 - p))/(n + 2)^2 + (1 - 2*p)^2/(n + 2)^2\n\n# plot of MSE(p-hat)\nplot(p, mse.phat,  \n     type = \"l\",\n     lwd =2,\n     col = \"firebrick2\",\n     main = \"Comparing MSE of Estimators for p when n=16\",\n     ylab = \"MSE\")\n\n# add plot of MSE(p-tilde)\nlines(p, mse.ptilde,  \n      lty=2, \n      lwd =2,\n      col = \"blue\")\n\n# add legend to plot\nlegend(0.37, 0.005, \n       legend=c(\"MSE(p.hat)\",\"MSE(p.tilde\"), \n       col=c(\"firebrick2\",\"blue\"), \n       lty=c(1,2), \n       ncol=1)\n```\n\n::: {.cell-output-display}\n![Comparing MSE of p-hat and p-tilde for X ~ Binom(16,p)](15-Properties-Estimators_files/figure-html/fig-mse-1.png){#fig-mse width=672}\n:::\n:::\n\n\n\n## Question 8 \n\n---\n\nUsing the plots of ${\\color{\\tomato}{\\mbox{MSE}(\\hat{p})}}$ and ${\\color{dodgerblue}{\\mbox{MSE}(\\tilde{p})}}$ in @fig-mse, identify the interval of $p$ values where the MSE of $\\tilde{p}$ is less than the MSE of $\\hat{p}$.\n\n\n### Solution to Question 8\n\n---\n\n<br>  \n<br>  \n<br>  \n\n\n\n## Question 9 \n\n---\n\nRun run the code below for different values of $n$ and say what happens to your choice of estimator as $n$ gets larger.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#########################################\n# adjust sample size n and run again\n# what happens to mse as n gets larger?\n#########################################\nn <- ??  # sample size\n\n####################################################\n# you do not need to edit the rest of the code cell\n####################################################\n\np <- seq(0, 1, length.out = 100)  # values of p\n\n# formula for MSE of p-hat\nmse.phat <- (p * (1 - p)) / n\n\n# formula for MSE of p-tilde\nmse.ptilde <- (n * p * (1 - p))/(n + 2)^2 + (1 - 2*p)^2/(n + 2)^2\n\n# plot of MSE(p-hat)\nplot(p, mse.phat,  \n     type = \"l\",\n     lwd =2,\n     col = \"firebrick2\",\n     main = \"Comparing MSE of Estimators for p\",\n     ylab = \"MSE\")\n\n# add plot of MSE(p-tilde)\nlines(p, mse.ptilde,  \n      lty=2, \n      lwd =2,\n      col = \"blue\")\n\n# add legend to plot\nlegend(0.37, 0.005, \n       legend=c(\"MSE(p.hat)\",\"MSE(p.tilde\"), \n       col=c(\"firebrick2\",\"blue\"), \n       lty=c(1,2), \n       ncol=1)\n```\n:::\n\n\n\n\n### Solution to Question 9\n\n---\n\n- Interpret the plots generated by the code above and answer the question.\n\n<br>  \n<br>  \n<br>  \n\n\n\n\n# Appendix: Proofs for Theorems {#sec-append}\n\n---\n\n\n### Theorem 15.1\n\n---\n\nIf $X_1$, $X_2$, $\\ldots$ , $X_n$ independently and identically distributed random variables, then  \n\n$$E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n E \\big[ X_i^2 \\big] - n E \\big[ \\overline{X}^2\\big] .$$\n\n#### Proof of Theorem 15.1\n\n---\n\nLet $X_1$, $X_2$, $\\ldots$ , $X_n$ be independently and identically distributed random variables with $\\displaystyle \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$. The following properties are used in the proof that follows.\n\n- The linearity of the expected value of random variables gives\n\n$$ E \\bigg[ \\sum_{i=1}^n (X_i) \\bigg] = \\sum_{i=1}^n  \\big( E \\lbrack X_i \\rbrack \\big) = n\\bar{X} \\quad \\mbox{and} \\quad  E \\bigg[ \\sum_{i=1}^n (X_i^2) \\bigg] = \\sum_{i=1}^n  \\big( E \\lbrack X_i^2 \\rbrack \\big).$$ {#eq-lin}\n\n- Recall properties of summation:\n\n$$\\sum_{i=1}^n (c a_i) = c \\big( \\sum_{i=1}^n a_i \\big) \\quad \\mbox{and} \\quad \\sum_{i=1}^n c = nc.$$ {#eq-sum}\n\n\nWe first expand the summation $\\sum_{i=1}^n (X_i - \\overline{X})^2$ inside the expected value \n\n$$\\begin{aligned}\nE\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] &= E \\bigg[ (X_1 - \\overline{X})^2 + (X_2 - \\overline{X})^2 + \\ldots + (X_n - \\overline{X})^2 \\bigg] \\\\\n&= E \\bigg[ (X_1^2 - 2X_1\\overline{X} + \\overline{X}^2) +(X_2^2 - 2X_2\\overline{X} + \\overline{X}^2) + \\ldots + (X_n^2 - 2X_n\\overline{X} + \\overline{X}^2)  \\bigg]\n\\end{aligned}$$\n\nRegrouping terms, using the linearity of the expected value and properties of summation stated above, we have \n\n$$E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] = \\sum_{i=1}^n E(X_i^2) + E \\bigg[ - 2 \\overline{X}\\left( {\\color{tomato}{\\sum_{i=1}^n X_i}} \\right)  + n \\overline{X}^2 \\bigg].$$\n\nSince $\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i$, we have ${\\color{tomato}{\\sum_{i=1}^n X_i = n \\overline{X}}}$, and therefore\n\n$$\\begin{aligned}\nE\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] &= \\sum_{i=1}^n E(X_i^2) + E \\bigg[ - 2 \\overline{X}\\left( {\\color{tomato}{n\\overline{X}}} \\right)  + n \\overline{X}^2 \\bigg] \\\\\n&=  \\sum_{i=1}^n E(X_i^2) + E \\bigg[ -n \\overline{X}^2\\bigg]\\\\\n&= \\sum_{i=1}^n E(X_i^2) - n E \\big[ \\overline{X}^2\\big] .\\\\\n\\end{aligned}$$\n\n\nThis concludes the proof!\n\n\n### Theorem 15.2\n\n---\n\nIf $X_1$, $X_2$, $\\ldots$ , $X_n$ are independently and identically distributed random variables with $E(X_i) = \\mu$ and $\\mbox{Var}(X_i) = \\sigma^2$, then $\\displaystyle E \\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] = (n-1)\\sigma^2$.\n\n\n#### Proof of Theorem 15.2\n\n---\n\nWe first apply [Theorem 15.1] to begin simplifying the expected value of the sum of the squared deviations. \n\n$$E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n {\\color{dodgerblue}{ E \\big[ X_i^2 \\big]}} - n {\\color{tomato}{E \\big[ \\overline{X}^2 \\big]}}$$\n\nFrom the variance property $\\mbox{Var}(Y) = E(Y^2) - \\big( E(Y) \\big)^2$, we know for any random variable $Y$, we have $E(Y^2) = \\mbox{Var}(Y) + \\big( E(Y) \\big)^2$. Applying this property to each $X_i$ and $\\overline{X}$ (which is a linear combination of random variables), we have\n\n$$E\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] =  \\sum_{i=1}^n \\bigg( {\\color{dodgerblue}{ \\mbox{Var} \\big[ X_i \\big] + \\left( E \\big[ X_i \\big] \\right)^2 }} \\bigg) - n \\left( {\\color{tomato}{\\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2}} \\right)$$\n\nFrom the first summation on the right side, we have ${\\color{dodgerblue}{ \\mbox{Var} \\big[ X_i \\big] + \\left( E \\big[ X_i \\big] \\right)^2 = \\sigma^2 + \\mu^2}}$. Recall from the Central Limit Theorem for Means, we have $\\mbox{Var} \\big[ \\overline{X} \\big] = \\frac{\\sigma^2}{n}$ and  $E \\big[  \\overline{X} \\big] = \\mu$, and thus we have ${\\color{tomato}{\\mbox{Var} \\big[ \\overline{X} \\big] + \\left( E \\big[  \\overline{X} \\big]\\right)^2 =\\frac{\\sigma^2}{n} + \\mu^2 }}$. Thus, we have\n\n$$\\begin{aligned}\nE\\bigg[ \\sum_{i=1}^n (X_i - \\overline{X})^2 \\bigg] &=  \\sum_{i=1}^n {\\color{dodgerblue}{ \\left( \\sigma^2 + \\mu^2 \\right)}} - n \\left( {\\color{tomato}{\\frac{\\sigma^2}{n}}} + {\\color{tomato}{ \\mu^2}} \\right) \\\\\n&= n(\\sigma^2 + \\mu^2) - \\sigma^2 - n\\mu^2 \\\\\n&= (n-1) \\sigma^2.\n\\end{aligned}$$\n\nThis concludes our proof!\n\n\n### Theorem 15.3\n\n----\n\nLet $\\hat{\\theta}$ be an estimator for parameter $\\theta$. The mean squared error (MSE) is\n\n$$\\mbox{MSE} \\big[ \\hat{\\theta} \\big] = \\mbox{Var} \\big[\\hat{\\theta} \\big] + \\left( \\mbox{Bias} \\big[ \\hat{\\theta}\\big] \\right)^2.$$\n\n#### Proof of Theorem 15.3\n\n---\n\n\nWe begin with the definition, add and subtract ${\\color{tomato}{E \\big[ \\hat{\\theta} \\big]}}$ inside the expected value, regroup terms inside the expected value, and finally break up the linear combination inside the expected value to get the result below.\n\n$$\\begin{aligned}\n\\mbox{MSE} \\big[ \\hat{\\theta} \\big] &= E \\big[ (\\hat{\\theta}-\\theta)^2 \\big] \\\\\n&= E \\bigg[ \\left( \\hat{\\theta} {\\color{tomato}{- E \\big[ \\hat{\\theta} \\big] + E \\big[ \\hat{\\theta} \\big]}} -\\theta \\right)^2 \\bigg] \\\\\n&= E \\bigg[ \\left( {\\color{tomato}{(\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big])}} + {\\color{dodgerblue}{(E \\big[ \\hat{\\theta} \\big] -\\theta)}} \\right)^2 \\bigg] \\\\\n&= E \\bigg[ {\\color{tomato}{(\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big])}}^2 + 2{\\color{tomato}{(\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big])}}{\\color{dodgerblue}{(E \\big[ \\hat{\\theta} \\big] -\\theta)}} +  {\\color{dodgerblue}{(E \\big[ \\hat{\\theta} \\big] -\\theta)}}^2 \\bigg] \\\\\n&=  E \\bigg[ (\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big])^2 \\bigg] + 2 E \\bigg[(\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big]) (E \\big[ \\hat{\\theta} \\big] -\\theta) \\bigg] + E \\bigg[ (E \\big[ \\hat{\\theta} \\big] -\\theta)^2 \\bigg]\n\\end{aligned}$$\n\n\nBy definition of the variance, we have\n\n$${\\color{tomato}{E \\bigg[ (\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big])^2 \\bigg] = \\mbox{Var} \\big[ \\hat{\\theta} \\big]}}.$$ {#eq-var-hat}\n\nBy definition of bias of an estimator, we have\n\n$${\\color{dodgerblue}{E \\bigg[ (E \\big[ \\hat{\\theta} \\big] -\\theta)^2 \\bigg] = E \\bigg[ \\big( \\mbox{Bias}(\\hat{\\theta}) \\big)^2 \\bigg] =  \\bigg( \\mbox{Bias}(\\hat{\\theta}) \\bigg)^2}}.$$ {#eq-bias-hat}\n\n- Note $\\mbox{Bias}(\\hat{\\theta})$ is a constant value which might be unknown, but it is not a random variable. \n- The expected value of a constant is the value of the constant, thus $E \\bigg[ \\big( \\mbox{Bias}(\\hat{\\theta}) \\big)^2 \\bigg] =  \\bigg( \\mbox{Bias}(\\hat{\\theta}) \\bigg)^2$.\n\n$${\\color{mediumseagreen}{\n\\begin{aligned}\nE \\bigg[ (\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big]) (E \\big[ \\hat{\\theta} \\big] -\\theta) \\bigg] &= E \\bigg[ \\hat{\\theta} \\cdot E \\big[ \\hat{\\theta} \\big] - \\hat{\\theta} \\cdot \\theta - \\bigg( E \\big[ \\hat{\\theta} \\big] \\bigg)^2 + E \\big[ \\hat{\\theta} \\big] \\cdot \\theta \\bigg] \\\\\n&=  E \\big[ \\hat{\\theta} \\big]  E \\big[ \\hat{\\theta} \\big]  - E \\big[ \\theta \\big] E \\big[ \\hat{\\theta} \\big] - \\bigg( E \\big[ \\hat{\\theta} \\big] \\bigg)^2 + E \\big[ \\theta \\big] E \\big[ \\hat{\\theta} \\big] \\\\\n&= 0.\n\\end{aligned}\n}}$$ {#eq-middle-stuff}\n\n\nUsing the results in (@eq-var-hat), (@eq-bias-hat), and (@eq-middle-stuff), we have\n\n$$\\begin{aligned}\n\\mbox{MSE} \\big[ \\hat{\\theta} \\big] &= {\\color{tomato}{E \\bigg[ (\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big])^2 \\bigg] }} + 2 {\\color{mediumseagreen}{ E \\bigg[(\\hat{\\theta} - E \\big[ \\hat{\\theta} \\big]) (E \\big[ \\hat{\\theta} \\big] -\\theta) \\bigg] }} + {\\color{dodgerblue}{E \\bigg[ (E \\big[ \\hat{\\theta} \\big] -\\theta)^2 \\bigg] }} \\\\\n&= {\\color{tomato}{\\mbox{Var} \\big[ \\hat{\\theta} \\big]}} + 2 \\cdot {\\color{mediumseagreen}{0}} + {\\color{dodgerblue}{\\left( \\mbox{Bias}(\\hat{\\theta}) \\right)^2}} \\\\\n&= \\mbox{Var} \\big[ \\hat{\\theta} \\big] + \\left( \\mbox{Bias}(\\hat{\\theta}) \\right)^2.\n\\end{aligned}$$\n\nThis completes our proof!\n\n\n\n---\n\n![Creative Commons License](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png) <nbsp>\n\n*Statistical Methods: Exploring the Uncertain* by [Adam Spiegler](https://github.com/CU-Denver-MathStats-OER/Statistical-Theory) is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\n\n\n\n",
    "supporting": [
      "15-Properties-Estimators_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
---
output:
  pdf_document: default
---

# Bootstrap Distributions Part 3

**Load required packages first**
```{r, message=FALSE}
library(resampledata)
```

## Verizon Repair Time Case Study

Verizon is the incumbent local exchange carrier (ILEC) for a large part of the Eastern US. When there is an emergency Verizon is responsible for making repairs for the customers of other telephone companies in the region known as competing local exchange carriers (CLEC's). Verizon is subject to fines if the repair times for CLEC customers are substantially worse than the times for Verizon customers. The dataset **Verizon** contains a sample of repair times (*Time*) for 1664 ILEC and 23 CLEC customers (*Group*). 

Rather than estimate the difference in mean times, suppose we look at the ratio of the means, what is the ratio of the ILEC mean repair time over the CLEC mean repair time?  **Construct a 95% bootstrap confidence interval for the ratio of the two means.**

```{r}
Time.ILEC <- subset(Verizon, select = ???, Group == ???, drop =T)
Time.CLEC <- subset(Verizon, select = ???, Group == ???, drop =T)

N <- 10^5
boot.ratio.mean <- numeric(N)
for (i in 1:N)
{
  ILEC.sample <- sample(???, ???, replace = ???)
  CLEC.sample <- sample(???, ???, replace = ???)
  boot.ratio.mean[i] <- ???
}

lower <- quantile(???, probs = ???)
upper <- quantile(???, probs = ???)
```

Comparing the observed ratio to the mean of the sampling distribution for the ratio of mean repair times.

```{r}
original.ratio <- ??? #observed sample stat
mean.boot <- ???    # Center of bootstrap

hist(???, main = "Bootstrap dist for ratio of means")
abline(v = mean.boot, col = "red", lty = 2, lwd = 1.5) # Bootstrap center
abline(v = original.ratio, col = "blue", lty = 2, lwd = 1.5) # observed stat
```

Is the shape of the sampling distribution for the ratio of means normal?

```{r}
qqnorm(???)
qqline(???)
```

## Section 5.6: Bias

* Let $\theta$ denote a population parameter. We denote an estimator for the parameter with a hat, $\widehat{\theta}$.

* An estimator $\hat{\theta}$ is **biased** if on average it tends to be too high or too low relative to the true value of $\theta$. The bias of an estimator is

$$ 
\mbox{Bias}\lbrack \hat{\theta} \rbrack = \mbox{E} \lbrack \hat{\theta} \rbrack - \theta. 
$$

* The bootstrap estimate of bias is
 
$$
\mbox{Bias}_{\rm{boot}} \lbrack \hat{\theta}^{\ast} \rbrack =  \mbox{E} \lbrack \hat{\theta}^{\ast} \rbrack - \hat{\theta}.
$$

+ $\mbox{E} \lbrack \hat{\theta}^{\ast} \rbrack$ denotes the center of the bootstrap distribution.

+ $\hat{\theta}$ denotes the sample statistic.

* An estimator is **unbiased if the bias is zero**.

### Estimating Bias from a Bootstrap Distribution

Using the output from the previous bootstrap distribution, calculate the bootstrap estimate of bias in the previous Verizon example.

We have $\mbox{E} \lbrack \hat{\theta}^{\ast} \rbrack =$ ???

We have $\hat{\theta} =$ ???

The bias is therefore $\mbox{Bias}_{\rm boot} \lbrack \hat{\theta}^{\ast} \rbrack =$ ???

### Rule of Thumb for Bias

* We can measure how extreme is the bias of an estimator using the ratio

$$
\frac{\mbox{Bias}}{\mbox{SE}} \approx  \frac{\mbox{Bootstrap Bias}}{\mbox{Bootstrap SE}}.
$$
* **Rule of Thumb:** If the ratio bias/SE exceeds $\pm 0.02$, then the bias is large enough to have a substantial effect on the accuracy of the estimate.

### Comparing Bias on Relative Terms

In the arsenic example  we used a bootstrap distribution to estimate the mean arsenic level (in ppb) present in groundwater in Bangladesh. The mean of the original sample is $\bar{x} = 125.320$. A mean of a bootstrap distribution is $125.229$. Then the bootstrap standard error is 17.9. **Which bias is more extreme, the bias in the arsenic or Verizon example?**

* For the arsenic example we have Bias/SE is 


* For the Verizon example we have Bias/SE is 


## Implementation and Accuracy

If we had a sample {10,  20 , 18}, how many possible bootstrap samples are there?

```{r}
n <- ???
choose(2*n-1, n)
```

In the Verizon example, there are at most $1664^{1664} \cdot 23^{23}$ different bootstrap samples, If there are duplicate values in the sample(s), then it gets even more tricky to avoid repeats.

* We have not been ensuring we generate all possible bootstrap samples while avoiding repeats.  
* We have used \textbf{Monte Carlo sampling} which gives an estimate of the theoretical bootstrap distribution.  
* The larger the number of bootstrap samples, the better the estimate. As a rule, $N=10^4$ bootstrap samples or more is sufficient.  

